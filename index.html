<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Angus Fung</title>
  
  <meta name="author" content="Angus Fung">
  <meta name="viewport" content="width=device-width, initial-scale=0.85">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Angus Fung</name>
              </p>
              <div class="profile-image-container">
                <a href="images/angus3.png"><img style="width:100%;max-width:200px;display:none;" alt="profile photo" src="images/angus3.png" class="hoverZoomLink mobile-only-image"></a>
              </div>
              <p>I completed my Ph.D at the <a href="https://robotics.utoronto.ca">University of Toronto, Robotics Institute</a>, advised by <a href="http://asblab.mie.utoronto.ca">Goldie Nejat</a>,
                where I work on robot perception and control using self-supervised learning and generative AI.
              </p>
              <p>Previously, I worked on learning algorithms at the <a href="https://vectorinstitute.ai">Vector Institute</a>
                 where I was advised by <a href="https://scholar.google.ca/citations?user=ymzxRhAAAAAJ&hl=en">Jimmy Ba</a>.
                 Currently, in my free-time, I am building <a href="https://www.syncereai.com">Syncere AI</a> with <a href="https://aarontan-git.github.io/">Aaron Tan</a> to bring consumer robots into society.
              </p>
                 <!-- I also worked on autonomous drone tracking and landing at the <a href="https://www.trailab.utias.utoronto.ca">Toronto Robotics and AI Lab</a> 
                 where I was advised by <a href="https://www.trailab.utias.utoronto.ca/stevenwaslander">Steven Waslander</a>. -->
              

              Outside of research:
              <ol style="margin-top: 0;">
                <li><strong>Music:</strong> I am active as a church organist having held positions at the <a href="https://www.metunited.org">Metropolitan United Church</a> (under <a href="https://music.utoronto.ca/our-people.php?fid=112">Dr. Patricia Wright</a>) and <a href="https://www.stmichaelscathedral.com/parish-team/">St. Michael's Cathedral Basilica</a>. 
                  I received my <a href="https://www.rcmusic.com/learning/examinations/recognizing-achievement/arct-lrcm">ARCT Diploma in Piano and Organ Performance</a> 
                 in 2013 at the <a href="https://www.rcmusic.com">Royal Conservatory of Music</a>.</li>
                <li><strong>Medicine:</strong> I build AI models to tackle open questions in neuroscience and neuro-ophthalmology with <a href="https://scholar.google.ca/citations?user=xm1qBO0AAAAJ&hl=en">Dr. Anthony Lang</a> and <a href="https://ophthalmology.utoronto.ca/faculty/edward-margolin">Dr. Edward Margolin</a>.<br></li>
                <li><strong>Gaming:</strong> I am a hobbyist game developer with experience at deploying games at scale, specifically in game design, load balancing, DDoS mitigation, security protocols, and anti-cheat mechanisms.</li>
                <li><strong>Startups:</strong> I co-founded <a href="https://scholarply.com">Scholarply</a> and <a href="http://3.230.3.191/">ONE800</a> with <a href="https://aarontan-git.github.io/">Aaron Tan</a>, leveraging LLMs for scholarship applications and personal companionship.</li>
                <li><strong>Entertainment:</strong> I worked with  2x Grammy Award recipient <a href="https://en.wikipedia.org/wiki/Sean_Leon">Sean Leon</a> to build AI
                  technology for their <a href="https://www.godsalgorithm.world">God's Algorithm</a> Project.</li> 
              </ol>
              
              <p>
                <em>Updated: 12/04</em>
              </p>

              <p style="text-align:center">
                <a href="mailto:angus.fung@mail.utoronto.ca">Email</a> &nbsp/&nbsp
                <a href="data/Angus_Fung_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.ca/citations?hl=en&user=QJeeFEMAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/angusfung/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%" class="desktop-image">
              <a href="images/angus3.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/angus3.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- New mini navigation bar -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align:center;">
              <a href="#research">Research</a> |
              <a href="#saas">SaaS Products</a> |
              <a href="#recognition">Recognition</a> |
              <a href="#teaching">Teaching</a> |
              <a href="#mentoring">Mentoring</a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading id="research">Research</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two">
                  <video width="100%" height="100%" muted autoplay loop playsinline style="object-fit: cover;">
                    <source src="images/mllm.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>MLLM-Search: A Zero-Shot Approach to Finding People using Multimodal Large Language Models</papertitle>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://aarontan-git.github.io">Aaron Hao Tan</a>,
              <a href="https://scholar.google.ca/citations?user=LA6TYrgAAAAJ&hl=en&authuser=1">Haitong Wang</a>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>,
              <br>
              <em>arXiv (Under Review)</em>, 2024
              <br>
              <a href="data/mllm-search.pdf">Paper</a>
              <p></p>
              <p>We present MLLM-Search, a novel multimodal language model approach to address the robotic person search problem under event-driven scenarios with incomplete or unavailable user schedules. Our method introduces zero-shot person search using language models for spatial reasoning, a novel visual prompting method generating topological graphs with semantic labels, and an MLLM-based search planner combining region and waypoint planning through our spatial chain-of-thought (SCoT) prompting method.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two">
                  <video width="100%" height="100%" muted autoplay loop playsinline style="object-fit: cover;">
                    <source src="images/finder.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Find Everything: A General Vision Language Model Approach to Multi-Object Search</papertitle>
              <br>
              <strong>Angus Fung</strong>,
              <a href="http://jeongwoongc.github.io">Daniel Choi</a>,
              <a href="https://scholar.google.ca/citations?user=LA6TYrgAAAAJ&hl=en&authuser=1">Haitong Wang</a>,
              <a href="https://aarontan-git.github.io">Aaron Hao Tan</a>
              <br>
              <em>arXiv (Under Review)</em>, 2024  
              <br>
              <a href="https://find-all-my-things.github.io/">Project Page</a>
              /
              <a href="data/finder.pdf">Paper</a>
              <p></p>
              <p>We present Finder, a novel approach to the multi-object search problem that leverages vision language models (VLMs) to efficiently locate multiple objects in diverse unknown environments. Our method combines semantic mapping with spatio-probabilistic reasoning and adaptive planning, improving object recognition and scene understanding through VLMs.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='project_video'>
                  <video width="100%" height="100%" muted autoplay loop playsinline style="object-fit: cover;">
                    <source src="images/ldtrack.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>LDTrack: Dynamic People Tracking by Service Robots using Diffusion Models</papertitle>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>, 
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>International Journal of Computer Vision (Minor Revision)</em>, 2024  
              <br>
              <a href="https://arxiv.org/pdf/2402.08774.pdf">Paper</a>
              <p></p>
              <p>We present a novel people tracking architecture for mobile service robots using conditional latent diffusion models, which we name Latent Diffusion Track (LDTrack), to solve the robotic problem of tracking multiple dynamic people under intraclass variations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='project_video'>
                  <video width="100%" height="100%" muted autoplay loop playsinline style="object-fit: cover;">
                    <source src="images/timclr.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Robots Autonomously Detecting People: A Multimodal Deep Contrastive Learning Method Robust to Intraclass Variations</papertitle>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>, 
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>IEEE Robotics and Autonomation Letters + IROS</em>, 2023  
              <br>
              <a href="https://arxiv.org/pdf/2203.00187">Paper</a> /
              <a href="https://www.youtube.com/watch?v=aP7BadmsuBo&t=4s">Talk</a> /
              <a href="data/timclr_abstract.pdf">Abstract</a> /
              <a href="data/timclr_poster.pdf">Poster</a>
              <p></p>
              <p>We present a novel multimodal person detection architecture to address the mobile robot problem of person detection under intraclass variations (e.g. partial occlusion, varying illumination, pose deformation) by introducing our Temporal Invariant Multimodal Contrastive Learning (TimCLR) method.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='project_video'>
                  <video width="100%" height="100%" muted autoplay loop playsinline style="object-fit: cover;">
                    <source src="images/cyb1.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>A Multi-Robot Person Search System for Finding Multiple Dynamic Users in Human-Centered Environments</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/sharaf-mohamed-03507972/?originalSubdomain=ca">Sharaf C Mohamed</a>,
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>IEEE Transactions on Cybernetics</em>, 2022  
              <br>
              <a href="http://asblab.mie.utoronto.ca/sites/default/files/FINAL%20VERSION.pdf">Paper</a> / 
              <a href="https://www.youtube.com/watch?v=hsF0qriqFMU">Video</a>
              <p></p>
              <p>We present a novel multi-robot person search system to generate search plans for multi-robot teams to find multiple dynamic users before a deadline.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='project_video'>
                  <video width="100%" height="100%" muted autoplay loop playsinline style="object-fit: cover;">
                    <source src="images/madd.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Robots Understanding Contextual Information in Human-Centered Environments using Weakly Supervised Mask Data Distillation</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/sharaf-mohamed-03507972/?originalSubdomain=ca">Daniel Dworakowski</a>,
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>International Journal of Computer Vision (IJCV)</em>, 2022  
              <br>
              <a href="https://link.springer.com/article/10.1007/s11263-022-01706-5">Paper</a>
              <p></p>
              <p>We present the novel Weakly Supervised Mask Data Distillation architecture for autonomously generating pseudo segmentation labels.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='project_video'>
                  <video width="100%" height="100%" muted autoplay loop playsinline style="object-fit: cover;">
                    <source src="images/acdcc.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM</papertitle>
              <br>
              <a href="https://scholar.google.ca/citations?user=ZWl8LsEAAAAJ&hl=en">Jason Rebello</a>,
              <strong>Angus Fung</strong>,
              <a href="https://www.trailab.utias.utoronto.ca/stevenwaslander">Steven Waslander</a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020  
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9197217">Paper</a>
              <p></p>
              <p>We present a method to calibrate the time-varying extrinsic transformation between any number of cameras and achieves measurement excitation over the entire configuration space of the mechanism resulting in a more accurate calibration.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='project_video'>
                  <video width="100%" height="100%" muted autoplay loop playsinline style="object-fit: cover;">
                    <source src="images/usar.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Using Deep Learning to Find Victims in Unknown Cluttered Urban Search and Rescue Environments</papertitle>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>, 
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>Springer Nature</em>, 2020  
              <br>
              <a href="https://link.springer.com/article/10.1007/s43154-020-00011-8">Paper</a>
              <p></p>
              <p>We investigate the first use of deep networks for victim identification in Urban Search and Rescue, for cases of partial occlusions and varying illumination, on a RGB-D dataset obtained by a mobile robot navigating cluttered USAR-like environments.</p>
            </td>
          </tr>
        </tbody></table>
        <br>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding: 0 20px;">
              <heading class="section-heading" id="saas">SaaS Products</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr class="saas-row">
            <td class="saas-logo">
              <img src='images/scholarply_logo.png' width="160">
            </td>
            <td class="saas-content">
              <div class="saas-header">
                <div class="saas-title">Scholarply</div>
                <div class="saas-founders">
                  <strong>Angus Fung (Founder)</strong>,
                  <a href="https://aarontan-git.github.io">Aaron Tan</a> (Founder)
                </div>
                <div class="saas-links">
                  2023 Q3-4 &nbsp;
                  <a href="https://scholarply.com/">Scholarply</a> / <a href="images/scholarply_newsletter.jpeg">Newsletter</a> / <a href="https://www.tiktok.com/@the.varsity/video/7306702650840583430?is_from_webapp=1&sender_device=pc&web_id=7322966712234640901">TikTok</a> / <a href="https://docs.google.com/presentation/d/1HqsTZXIIeu7J2EkMgpsmMI947YwvwM3sJgxDR9XeKWc/edit?usp=share_link">Pitch Deck</a> / <a href="https://youtu.be/vcr5VihgeBQ">Demo</a>
                  <br>
                </div>
              </div>
              <ul>
                <li>Accelerating the scholarship application process via LLM agents to help students secure funding while focusing on their studies.</li>
                <li>Selected by Microsoft Startup Hub Program, receiving grants worth $150k.</li>
              </ul>
            </td>
          </tr>

          <!-- Repeat this structure for each SaaS product -->
          <tr class="saas-row">
            <td class="saas-logo">
              <img src='images/one800_logo.png' width="160">
            </td>
            <td class="saas-content">
              <div class="saas-header">
                <div class="saas-title">ONE800</div>
                <div class="saas-founders">
                  <strong>Angus Fung (Founder)</strong>,
                  <a href="https://aarontan-git.github.io">Aaron Tan</a> (Founder)
                </div>
                <div class="saas-links">
                  2023 Q1-2 &nbsp;
                  <a href="http://3.230.3.191/">ONE800</a> / <a href="https://twitter.com/one800chat">Twitter</a> / <a href="https://www.instagram.com/one800chat/">Instagram</a> / <a href="data/one800_deck.pdf">Pitch Deck</a> / <a href="https://youtu.be/Eq4MWiPGJ5s?si=Ps890MJe8P8nZaSv">Demo</a>
                </div>
              </div>
              <ul>
                <li>An all-in-one service built in to iMessage aimed at lowering the barrier of entry for LLMs and Generative AI.</li>
                <li>Launched multi-modal conversations with proprietary model 4 months before GPT-4V.</li>
                <li>Gained significant traction with thousands of monthly active users.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <br>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding: 0 20px;">
              <heading class="section-heading" id="recognition">Recognition</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>2024</strong>: Doctoral Completion Award ($4k) <br>
          <strong>2024</strong>: LocalHost Fellowship ($3k) <br>
          <strong>2024</strong>: Microsoft Startup Hub Program ($150k) <br>
          <strong>2023</strong>: Ontario Graduate Scholarship - University of Toronto ($15k) <br>
          <strong>2022</strong>: Rimrott Memorial Graduate Scholarship - University of Toronto ($4k) <br>
          <strong>2021</strong>: RO-MAN Roboethics Competition, McGill University - 1st Place ($1k) <br>
          <strong>2021</strong>: University of Toronto MIE Fellowship ($14k) <br>
          <strong>2020</strong>: Queen Elizabeth II Graduate Scholarship - University of Toronto ($15k) <br>
          <strong>2020</strong>: University of Toronto MIE Fellowship ($14k) <br>
          <strong>2019</strong>: University of Toronto MIE Fellowship ($14k) <br>
          <strong>2019</strong>: Healthcare Robotics NSERC Fellowship ($10k) <br>
          <strong>2014-2018</strong>: Dean's Honour List <br>
          <strong>2014</strong>: Delta Tau Delta Award ($3k) <br>
          <strong>2014</strong>: University of Toronto Scholars (Academic Excellence) ($7.5k) <br>
          <strong>2014</strong>: University of Toronto Scholar ($5k) <br>          
          <strong>2013</strong>: ARCT Diploma - Piano Performance <br>
          <strong>2013</strong>: ARCT Diploma - Organ Performance <br>
        </td>
        </tbody></table>

        <br>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding: 0 20px;">
              <heading class="section-heading" id="teaching">Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>2024F</strong>: ROB501: Computer Vision for Robotic, TA, University of Toronto <br>
          <strong>2024W</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
          <strong>2023F</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
          <strong>2022F</strong>: ROB501: Computer Vision for Robotic, TA, University of Toronto <br>
          <strong>2022W</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
          <strong>2021W</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
          <strong>2020W</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
        </td>
        </tbody></table>
        <br>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding: 0 20px;">
              <heading class="section-heading" id="mentoring">Mentoring</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>2023-2024</strong>: Undergraduate Thesis Student: Michelle Quan <a href="data/michelle_thesis.pdf">(Thesis)</a><br>
          <strong>2022-2023</strong>: Undergraduate Thesis Student: Grace Bae <a href="data/grace_thesis.pdf">(Thesis)</a><br>
          <strong>2021-2022</strong>: Undergraduate Thesis Student: Giro Ele <a href="data/giro_thesis.pdf">(Thesis)</a><br>
        </td>
        </tbody></table>
        <br>


      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
            </p>
          </td>
        </tr>
      </tbody></table>

      </td>
    </tr>
  </table>

  <!-- ClusterMaps widget -->
  <div id="clustrmaps-widget-container">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=t-ol8tyYgBLUCP0VDJGKpipLcbGmyjL2HdcOLVrAYl0&cl=ffffff&w=a"></script>
  </div>

</body>

</html>