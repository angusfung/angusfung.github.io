<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Angus Fung</title>
  
  <meta name="author" content="Angus Fung">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Angus Fung</name>
              </p>
              <p>I am a Ph.D candidate at the <a href="https://robotics.utoronto.ca">University of Toronto, Robotics Institute</a>, where I am advised by <a href="http://asblab.mie.utoronto.ca">Goldie Nejat</a>.  My research is at the intersection of robotics, machine learning, and computer vision, where I am developing robust object detectors which learn invariant features to detect people in human-centered environments.
              </p>
              <p>During my undergraduate, in <a href="https://engsci.utoronto.ca/">Engineering Science</a>, I worked on distributed learning algorithms at the <a href="https://vectorinstitute.ai">Vector Institute</a> where I was advised by <a href="https://jimmylba.github.io">Jimmy Ba</a>. I also worked on autonomous drone tracking and landing at the <a href="https://www.trailab.utias.utoronto.ca">Toronto Robotics and AI Lab</a> where I was advised by <a href="https://www.trailab.utias.utoronto.ca/stevenwaslander">Steven Waslander</a>. I was also part of the <a href="https://www.autodrive.utoronto.ca/">University Of Toronto Self-Driving Car Team</a>.
              </p>
              <p>
              Outside of research, I am working with  2x Grammy Award recipient <a href="https://en.wikipedia.org/wiki/Sean_Leon">Sean Leon</a> to build AI
              technology for their <a href="https://www.herdimmunity.info">Herd Immunity</a> and <a href="https://www.godsalgorithm.world">God's Algorithm</a> Project. 
              I am also active as a church organist having held positions at the <a href="https://www.metunited.org">Metropolitan United Church</a> (under <a href="https://music.utoronto.ca/our-people.php?fid=112">Dr. Patricia Wright</a>) 
              and <a href="https://www.stmichaelscathedral.com/parish-team/">St. Michael's Cathedral Basilica</a>. I received my <a href="https://www.rcmusic.com/learning/examinations/recognizing-achievement/arct-lrcm">ARCT</a> 
              Diploma in Piano and Organ Performance in 2013 at the <a href="https://www.rcmusic.com">Royal Conservatory</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:angus.fung@mail.utoronto.ca">Email</a> &nbsp/&nbsp
                <a href="data/Angus_Fung_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.ca/citations?hl=en&user=QJeeFEMAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/angusfung/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/angus.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/angus.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/timclr.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="https://arxiv.org/abs/2203.00187"> -->
                <a href="data/timclr_r2.pdf">
                <papertitle>Robots Autonomously Detecting People: A Multimodal Deep Contrastive Learning Method Robust to Intraclass Variations</papertitle>
              </a>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>, 
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>Submitted</em>, 2022  
              <br>
							<!-- <a href="https://arxiv.org/abs/2203.00187">arXiv</a> -->
              <a href="data/timclr_r2.pdf">Paper</a>
							<!-- <a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> / -->
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We present a novel multimodal person detection architecture to address the mobile robot problem of person detection under intraclass variations (e.g. partial occlusion, varying illumination, pose deformation) by introducing our Temporal Invariant Multimodal Contrastive Learning (TimCLR) method.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/cyb1.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="http://asblab.mie.utoronto.ca/sites/default/files/FINAL%20VERSION.pdf">
                <papertitle>A Multi-Robot Person Search System for Finding Multiple Dynamic Users in Human-Centered Environments</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/sharaf-mohamed-03507972/?originalSubdomain=ca">Sharaf C Mohamed</a>,
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>IEEE Transactions on Cybernetics</em>, 2022  
              <br>
							<a href="http://asblab.mie.utoronto.ca/sites/default/files/FINAL%20VERSION.pdf">Paper</a> / 
							<a href="https://www.youtube.com/watch?v=hsF0qriqFMU">Video</a>
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We present a novel multi-robot person search system to generate search plans for multi-robot teams to find multiple dynamic users before a deadline.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/madd.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2012.08282.pdf">
                <papertitle>Robots Understanding Contextual Information in Human-Centered Environments using Weakly Supervised Mask Data Distillation</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/sharaf-mohamed-03507972/?originalSubdomain=ca">Daniel Dworakowski</a>,
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>International Journal of Computer Vision (IJCV)</em>, 2022  
              <br>
							<a>In Press </a>
							<!-- <a href="https://www.youtube.com/watch?v=hsF0qriqFMU">video</a> -->
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We present the novel Weakly Supervised Mask Data Distillation (WeSuperMaDD) architecture for autonomously generating pseudo segmentation labels.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/acdcc.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/acdcc.PNG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://ieeexplore.ieee.org/abstract/document/9197217">
                <papertitle>AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ca/citations?user=ZWl8LsEAAAAJ&hl=en">Jason Rebello</a>,
              <strong>Angus Fung</strong>,
              <a href="https://www.trailab.utias.utoronto.ca/stevenwaslander">Steven Waslander</a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020  
              <br>
							<a href="https://ieeexplore.ieee.org/abstract/document/9197217">Paper</a>
							<!-- <a href="https://www.youtube.com/watch?v=hsF0qriqFMU">video</a> -->
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We present a method to calibrate the time-varying extrinsic transformation between any number of cameras and achieves measurement excitation over the entire configuration space of the mechanism resulting in a more accurate calibration.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/usar.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/acdcc.PNG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://link.springer.com/article/10.1007/s43154-020-00011-8">
                <papertitle>Using Deep Learning to Find Victims in Unknown Cluttered Urban Search and Rescue Environments</papertitle>
              </a>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>, 
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>Springer Nature</em>, 2020  
              <br>
							<a href="https://link.springer.com/article/10.1007/s43154-020-00011-8">Paper</a>
							<!-- <a href="https://www.youtube.com/watch?v=hsF0qriqFMU">video</a> -->
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We investigate the first use of deep networks for victim identification in Urban Search and Rescue, for cases of partial occlusions and varying illumination, on a RGB-D dataset obtained by a mobile robot navigating cluttered USAR-like environments.</p>
            </td>
          </tr>

        </tbody></table>

				

      </td>
    </tr>
  </table>
</body>

</html>
