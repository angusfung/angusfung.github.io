<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Angus Fung</title>
  
  <meta name="author" content="Angus Fung">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Angus Fung</name>
              </p>
              <p>I am a Ph.D candidate at the <a href="https://robotics.utoronto.ca">University of Toronto, Robotics Institute</a>, where I am advised by <a href="http://asblab.mie.utoronto.ca">Goldie Nejat</a>.  
                My research is at the intersection of robotics, machine learning, and computer vision, where I am develop perception algorithms that are robust under challenging human-centered environments.
              </p>
              <p>During my undergraduate, in <a href="https://engsci.utoronto.ca/">Engineering Science</a>, I worked on distributed learning algorithms at the <a href="https://vectorinstitute.ai">Vector Institute</a>
                 where I was advised by <a href="https://jimmylba.github.io">Jimmy Ba</a>. I also worked on autonomous drone tracking and landing at the <a href="https://www.trailab.utias.utoronto.ca">Toronto Robotics and AI Lab</a> 
                 where I was advised by <a href="https://www.trailab.utias.utoronto.ca/stevenwaslander">Steven Waslander</a>.
              </p>

              Outside of research:
              <ol style="margin-top: 0;">
                <li>I am the founder of <a href="https://one800chat.com">ONE800</a> (with <a href="https://aarontan-git.github.io/">Aaron Tan</a>), a personal companion that leverages multimodal LLM agents.</li>
                <li>I am the founder of <a href="https://scholarply.com">Scholarply</a> (with <a href="https://aarontan-git.github.io/">Aaron Tan</a>), which automates the scholarship application process through LLM agents.</li> 
                <li>I worked with  2x Grammy Award recipient <a href="https://en.wikipedia.org/wiki/Sean_Leon">Sean Leon</a> to build AI
                  technology for their <a href="https://www.herdimmunity.info">Herd Immunity</a> and <a href="https://www.godsalgorithm.world">God's Algorithm</a> Project.</li> 
                <li>I am active as a church organist having held positions at the <a href="https://www.metunited.org">Metropolitan United Church</a> (under <a href="https://music.utoronto.ca/our-people.php?fid=112">Dr. Patricia Wright</a>) 
                  and <a href="https://www.stmichaelscathedral.com/parish-team/">St. Michael's Cathedral Basilica</a>. I received my <a href="https://www.rcmusic.com/learning/examinations/recognizing-achievement/arct-lrcm">ARCT</a> 
                  Diploma in Piano and Organ Performance in 2013 at the <a href="https://www.rcmusic.com">Royal Conservatory of Music</a>.</li>
                <li>I am building a patient screen tool using LLM agents with Dr. <a href="https://torontoeye.ca/dr-edward-margolin/">Edward Margolin</a>.<br></li>
                <li>I am a hobbyist game developer with experience at deploying games at scale, specifically in game design, load balancing, DDoS mitigation, cloud-based and firewall security measures, and anti-cheat mechanisms.</li>
              </ol>
              
              <p style="text-align:center">
                <a href="mailto:angus.fung@mail.utoronto.ca">Email</a> &nbsp/&nbsp
                <a href="data/Angus_Fung_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.ca/citations?hl=en&user=QJeeFEMAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/angusfung/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/angus.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/angus.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>


      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/ldtrack.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2203.00187"> -->
                <a href="data/timclr_r2.pdf">
                <papertitle>LDTrack: Dynamic People Tracking by Service Robots using Diffusion Models</papertitle>
              </a>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>, 
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>arXiV (Submitted)</em>, 2024  
              <br>
              <a href="https://arxiv.org/pdf/2402.08774.pdf">Paper</a>
              <!-- <a href="https://www.youtube.com/watch?v=aP7BadmsuBo&t=4s">Talk</a> / -->
              <!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
              <!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We present a novel people tracking architecture for mobile service robots using conditional latent diffusion models, which we name Latent Diffusion Track (LDTrack), to solve the robotic problem of tracking multiple dynamic people under intraclass variations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/timclr.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="https://arxiv.org/abs/2203.00187"> -->
                <a href="https://arxiv.org/pdf/2203.00187">
                <papertitle>Robots Autonomously Detecting People: A Multimodal Deep Contrastive Learning Method Robust to Intraclass Variations</papertitle>
              </a>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>, 
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>IEEE Robotics and Autonomation Letters + IROS</em>, 2023  
              <br>
              <a href="https://arxiv.org/pdf/2203.00187">Paper</a> /
              <a href="https://www.youtube.com/watch?v=aP7BadmsuBo&t=4s">Talk</a> /
              <a href="data/timclr_abstract.pdf">Abstract</a> /
              <a href="data/timclr_poster.pdf">Poster</a>
              <p></p>
              <p>We present a novel multimodal person detection architecture to address the mobile robot problem of person detection under intraclass variations (e.g. partial occlusion, varying illumination, pose deformation) by introducing our Temporal Invariant Multimodal Contrastive Learning (TimCLR) method.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/cyb1.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="http://asblab.mie.utoronto.ca/sites/default/files/FINAL%20VERSION.pdf">
                <papertitle>A Multi-Robot Person Search System for Finding Multiple Dynamic Users in Human-Centered Environments</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/sharaf-mohamed-03507972/?originalSubdomain=ca">Sharaf C Mohamed</a>,
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>IEEE Transactions on Cybernetics</em>, 2022  
              <br>
							<a href="http://asblab.mie.utoronto.ca/sites/default/files/FINAL%20VERSION.pdf">Paper</a> / 
							<a href="https://www.youtube.com/watch?v=hsF0qriqFMU">Video</a>
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We present a novel multi-robot person search system to generate search plans for multi-robot teams to find multiple dynamic users before a deadline.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/madd.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2012.08282.pdf">
                <papertitle>Robots Understanding Contextual Information in Human-Centered Environments using Weakly Supervised Mask Data Distillation</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/sharaf-mohamed-03507972/?originalSubdomain=ca">Daniel Dworakowski</a>,
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>International Journal of Computer Vision (IJCV)</em>, 2022  
              <br>
							<a href="https://link.springer.com/article/10.1007/s11263-022-01706-5">Paper</a>
							<!-- <a href="https://www.youtube.com/watch?v=hsF0qriqFMU">video</a> -->
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We present the novel Weakly Supervised Mask Data Distillation (WeSuperMaDD) architecture for autonomously generating pseudo segmentation labels.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/acdcc.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/acdcc.PNG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://ieeexplore.ieee.org/abstract/document/9197217">
                <papertitle>AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ca/citations?user=ZWl8LsEAAAAJ&hl=en">Jason Rebello</a>,
              <strong>Angus Fung</strong>,
              <a href="https://www.trailab.utias.utoronto.ca/stevenwaslander">Steven Waslander</a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020  
              <br>
							<a href="https://ieeexplore.ieee.org/abstract/document/9197217">Paper</a>
							<!-- <a href="https://www.youtube.com/watch?v=hsF0qriqFMU">video</a> -->
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We present a method to calibrate the time-varying extrinsic transformation between any number of cameras and achieves measurement excitation over the entire configuration space of the mechanism resulting in a more accurate calibration.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/usar.mov" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/acdcc.PNG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://link.springer.com/article/10.1007/s43154-020-00011-8">
                <papertitle>Using Deep Learning to Find Victims in Unknown Cluttered Urban Search and Rescue Environments</papertitle>
              </a>
              <br>
              <strong>Angus Fung</strong>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>, 
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>Springer Nature</em>, 2020  
              <br>
							<a href="https://link.springer.com/article/10.1007/s43154-020-00011-8">Paper</a>
							<!-- <a href="https://www.youtube.com/watch?v=hsF0qriqFMU">video</a> -->
							<!-- <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> /  -->
							<!-- <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				 -->
              <p></p>
              <p>We investigate the first use of deep networks for victim identification in Urban Search and Rescue, for cases of partial occlusions and varying illumination, on a RGB-D dataset obtained by a mobile robot navigating cluttered USAR-like environments.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>SaaS (Software as a Service) Products</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/scholarply_logo.png' width="160">
              </div>
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Scholarply</papertitle>
                <br>
                <strong>Angus Fung (Founder)</strong>,
                <a href="https://aarontan-git.github.io">Aaron Tan</a> (Founder),
                <br>
                2023 &nbsp
                <br>
                  2023 Q3-4 &nbsp
                  <br>
                  <a href="https://scholarply.com/">Scholarply</a> / <a href="images/scholarply_newsletter.jpeg">Newsletter</a> / <a href="https://www.tiktok.com/@the.varsity/video/7306702650840583430?is_from_webapp=1&sender_device=pc&web_id=7322966712234640901">TikTok</a>
                  <br>
                  <a href="https://docs.google.com/presentation/d/1HqsTZXIIeu7J2EkMgpsmMI947YwvwM3sJgxDR9XeKWc/edit?usp=share_link">Pitch Deck</a> / <a href="https://youtu.be/vcr5VihgeBQ">Demo</a>
                  <br>

                <ul>
                  <li>Accelerating the scholarship application process via LLM agents to help students secure funding while focusing on their studies.</li>
                  <li>Selected by Microsoft Startup Hub Program, receiving grants worth $150k.</li>
                  <li>Succesfully raised at $1M Valuation.</li>
                </ul>

                </td>
            </tr>            
          <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/one800_logo.png' width="160">
              </div>
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>ONE800</papertitle>
                <br>
                <strong>Angus Fung (Founder)</strong>,
                <a href="https://aarontan-git.github.io">Aaron Tan</a> (Founder),
                <br>
                2023 Q1-2 &nbsp
                <br>
                <a href="https://efe359.myshopify.com/">ONE800</a> / <a href="https://twitter.com/one800chat">Twitter</a> / <a href="https://www.instagram.com/one800chat/">Instagram</a>
                <br>
                <a href="data/one800_deck.pdf">Pitch Deck</a> / <a href="https://youtu.be/Eq4MWiPGJ5s?si=Ps890MJe8P8nZaSv">Demo</a>
                <br>

                <ul>
                  <li>An all-in-one service built in to iMessage aimed at lowering the barrier of entry for LLMs and Generative AI.</li>
                  <li>Launched multi-modal conversations with proprietary model 4 months before GPT-4V.</li>
                  <li>Gained significant traction with thousands of monthly active users.</li>
                </ul>
                </td>
            </tr>

        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Recognition</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>2023</strong>: Microsoft Startup Hub Program ($150k) <br>
          <strong>2023</strong>: Ontario Graduate Scholarship - University of Toronto ($15k) <br>
          <strong>2022</strong>: Rimrott Memorial Graduate Scholarship - University of Toronto ($4k) <br>
          <strong>2021</strong>: RO-MAN Roboethics Competition, McGill University - 1st Place ($1k) <br>
          <strong>2021</strong>: University of Toronto MIE Fellowship ($14k) <br>
          <strong>2020</strong>: Queen Elizabeth II Graduate Scholarship - University of Toronto ($15k) <br>
          <strong>2020</strong>: University of Toronto MIE Fellowship ($14k) <br>
          <strong>2019</strong>: University of Toronto MIE Fellowship ($14k) <br>
          <strong>2019</strong>: Healthcare Robotics NSERC Fellowship <br>
          <strong>2014-2018</strong>: Dean's Honour List <br>
          <strong>2014</strong>: Delta Tau Delta Award ($3k) <br>
          <strong>2014</strong>: University of Toronto Scholars (Academic Excellence) ($7.5k) <br>
          <strong>2014</strong>: University of Toronto Scholar ($5k) <br>          
          <strong>2013</strong>: ARCT Diploma - Piano Performance <br>
          <strong>2013</strong>: ARCT Diploma - Organ Performance <br>
        </td>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>2024W</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
          <strong>2023F</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
          <strong>2022F</strong>: ROB501: Computer Vision for Robotic, TA, University of Toronto <br>
          <strong>2022W</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
          <strong>2021W</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
          <strong>2020W</strong>: MIE443: Mechatronics Systems: Design & Integration Head Tutorial TA, University of Toronto <br>
        </td>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Mentoring</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>2023-2024</strong>: Undergraduate Thesis Student: Michelle Quan <a href="data/michelle_thesis.pdf">(Thesis)</a><br>
          <strong>2023-2024</strong>: Undergraduate Thesis Student: Grace Bae <a href="data/grace_thesis.pdf">(Thesis)</a><br>
          <strong>2023-2024</strong>: Undergraduate Thesis Student: Giro Ele <a href="data/giro_thesis.pdf">(Thesis)</a><br>
        </td>
        </tbody></table>


      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
            </p>
          </td>
        </tr>
      </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
